Production-Scale Roadmap for “thesis” to 100M Users

Version: 1.0
Owner: Architecture Guild
Last updated: 2025-09-29

Executive Summary
- Goal: Evolve the current “thesis” monolith into a globally scalable, highly-available, multi-region platform capable of handling 100M users with real-time updates and heavy 3D assets.
- Strategy:
  1) Decompose into domain-driven microservices with gRPC for internal comms and REST/GraphQL at the edge.
  2) Use Go for high-throughput, latency-sensitive services; C++ for hot-path optimization (e.g., OR-Tools); Node/TypeScript for API/libs; Python for ML pipelines.
  3) Event-driven architecture with Kafka; Redis for caching; Aurora Postgres for OLTP; OpenSearch for search; Snowflake (or Spark/EMR) for analytics.
  4) Real-time at scale via dedicated WebSocket service (Go) backed by pub/sub (NATS or Kafka+Redis fanout).
  5) Micro-frontend strategy with federation; standardize frontend on React Query v5 and Redux Toolkit; aggressively optimize 3D delivery.
  6) Infrastructure via AWS CDK; Kubernetes (EKS) or ECS; multi-account, multi-region; observability with OpenTelemetry, Prometheus, Grafana, Sentry.
  7) Security: OIDC, mTLS, WAF, rate limits at edge, secret management, data encryption, privacy compliance.
  8) Phased migration using the strangler fig pattern; parallel run; continuous load testing and chaos engineering.

Current State Assessment (from repository)
- Backend (Node/TS, Express):
  - Entry: backend/src/index.ts mounts routes: user, profile, foods, cart, orders, driver, food-allocation-milp. Rate limiting middleware and CORS enabled.
  - DB: PostgreSQL via backend/src/db/util.ts creates a new Pool per query (inefficient). Needs singleton pool, connection pooling, retry logic, circuit breakers.
  - Redis: backend/src/redisClient.ts connects to redis://localhost:6379 with no TLS/auth; should be configurable, clustered, secure, and injected.
  - WebSockets: backend/src/websocket/trackingServer.ts is currently commented placeholder using ws and Redis sets. socket.io dependency present but not wired. Real-time not production-ready.
  - Optimization: backend/src/routes/food-allocation-milp.ts builds input JSON, writes a Python script dynamically, and spawns python with OR-Tools, then reads a JSON output file. This is brittle and not horizontally scalable. Should be a separate service (Go/C++) with gRPC, job queue, and resource isolation.
  - Services include cartService, orderService, mapService (Google Maps geocode), s3Service, userService.
  - Tables cover donors, NGOs, food donations, allocations, drivers, routes, chat, orders, etc.
- ML/Feature work:
  - backend/fwp/ contains Python models and data (rf.pkl, pol.pkl, vectorizer.py, classification helpers). Ad-hoc ML artifacts stored in repo; should be moved to versioned model registry/storage.
- Frontend (Vite, React TS, Three.js):
  - Dependencies include both @tanstack/react-query v5 and react-query v3—conflict to resolve. Should standardize on @tanstack/react-query v5.
  - Heavy 3D assets (.glb, .fbx) under public/, numerous, large. Needs compression (gltfpack/meshoptimizer/ktx2), CDN, LOD, lazy loading, and streaming strategies.
  - UI stacks: shadcn, tailwind-merge. Maps: @react-google-maps/api and Leaflet both present—standardize to one.
- Documentation exists for backend routes/tables; good start for domain inventory.
- Ops/Infra: No IaC yet (CDK/Terraform). No CI/CD pipelines found. No containerization manifest in backend package.json.

Non-Functional Requirements (NFRs) and SLOs
- Availability: 99.99% (four-nines) for core APIs; 99.9% for real-time channels globally.
- Latency p95: 
  - Read APIs: <200ms intra-region, <400ms cross-region.
  - Write APIs: <300ms intra-region.
  - Real-time fanout: <250ms edge-to-edge for subscribed recipients.
- Throughput: Design baseline for peak 500k RPS across services; burst resilience with autoscaling and backpressure.
- Data durability: RPO ≤ 1 minute, RTO ≤ 15 minutes for critical data; multi-AZ, multi-region DR.
- Security: Zero-trust network, mTLS/MTLS, OAuth2/OIDC, WAF, DDoS protection, least privilege, secrets rotation.
- Cost: FinOps governance; 50th percentile cost per 1k requests target defined in FinOps appendix (to be refined).
- Observability: 100% tracing sampled intelligently, metrics SLO dashboards, centralized structured logging.

Target Architecture Overview
- Edge:
  - CloudFront + WAF + Route53. API Gateway (REST, optionally WebSocket)-or ALB/NLB for gRPC/HTTP2.
  - CDN for static content and 3D assets (HTTP range requests, Brotli, ktx2 textures).
- API Gateway Layer:
  - Public APIs: REST/GraphQL (BFFs per frontend or a federated GraphQL).
  - Authentication/Authorization via OIDC (Cognito/Auth0/Keycloak).
- Internal Mesh:
  - Microservices on EKS (or ECS) with a service mesh (Istio/Linkerd) enabling mTLS, retries, traffic policies, gRPC routing.
  - Inter-service communication: gRPC for sync calls; Kafka for async events.
- Data:
  - OLTP: Amazon Aurora PostgreSQL (Serverless v2) with read replicas, partitioning, multi-tenant/key-based sharding when needed.
  - Caching: ElastiCache Redis Cluster with TLS/auth; use as caching, sessions (if needed), job queues (if not Kafka for jobs).
  - Search: Amazon OpenSearch Service (Elasticsearch compatible) for text/geo queries and analytics search.
  - Object Storage: S3 for media/models with lifecycle rules and signed URL access.
  - Analytics: Snowflake for warehouse and BI, or Delta Lake (S3 + EMR/Spark/Databricks). CDC from Postgres to Kafka to Snowflake.
- Realtime:
  - Dedicated Real-Time Gateway Service (Go) managing WebSocket connections (Gorilla/Websocket or Centrifuge/Centrifugo). Backplane: NATS JetStream or Kafka for large fanout; Redis pub/sub for local node fanout only.
  - Optional managed alternatives: AWS API Gateway WebSocket, Ably, or Pusher for offloading global websockets.
- Optimization & Compute:
  - Allocation/Optimization Service: C++ (OR-Tools) gRPC service in isolated pool; orchestrated by an API that validates inputs and posts jobs to a queue; returns job status and results.
  - ML Services: Python microservices for classification; batch training pipelines in EMR/Databricks/SageMaker; model registry (MLflow).
- Observability:
  - OpenTelemetry SDKs in all services; collectors in-cluster to Prometheus/Grafana and OpenSearch; traces exported to Tempo/Jaeger/X-Ray.
  - Sentry for FE/BE error tracking.
- Security:
  - Vault/Secrets Manager for secrets and KMS encryption; mTLS; WAF; Shield Advanced; CIS benchmarks; image scanning.

Service Decomposition (Domains and Responsibilities)
- Identity Service (Node/Go):
  - OIDC provider integration (Cognito/Keycloak). Issue/validate JWTs. RBAC/ABAC policies. 2FA/OTP via Twilio.
- User Profile Service:
  - Manage user profiles, NGO associations, donors, recipients; GDPR delete requests.
- Donor Service:
  - Donor onboarding, verification, donation listings CRUD, media management (S3).
- NGO Service:
  - NGO verification, capacity management, priority, food preferences, geo profiles.
- Allocation Orchestrator API (Node/Go):
  - Accept allocation requests; validate state; enqueue jobs; aggregate results; idempotency and audit logs.
- Optimization Engine (C++/Go with OR-Tools) [gRPC]:
  - Hot path solver; supports multi-criteria optimization; configurable constraints; returns allocations with objective metrics.
- Orders & Fulfillment Service:
  - Track allocations → orders → pickup/delivery → completion; state machine; compensation/rollback.
- Driver/Logistics Service (Go):
  - Driver auth, live location ingest (WebSocket or HTTP), route building; integration with map/routing service.
- Routing/Maps Service (Go/Node):
  - Abstracts Google Maps/OSRM/Valhalla; caching and rate limiting.
- Real-Time Gateway (Go):
  - WebSocket multiplexor; topics per order, driver, NGO; fanout via NATS/Kafka; presence; backpressure and quotas.
- Notification Service:
  - Email/SMS/Push (SES/Twilio/FCM/APNS); templates, rate limits, delivery status.
- Media/Assets Service:
  - S3 signed upload/download; 3D asset processing pipeline triggers.
- Search Service (OpenSearch):
  - Index donations, NGOs, drivers; full-text, geo, faceted search.
- Chat/Communication Service:
  - 1:1 or group; persistence; moderation; compliance retention.
- Analytics/Events Service:
  - Event collection (Kafka), CDC, Snowflake ingestion; BI metrics surfaces.
- Admin Service:
  - Admin policies, audit logs, backoffice dashboards.
- API Gateway/BFFs:
  - Per-frontend BFFs to tailor payloads and caching (Node or Go), or a federated GraphQL gateway.

Data Architecture
- OLTP: Aurora Postgres Serverless v2; schema normalized; use partitioning for large append-only tables (driver_location_history, logs).
- Sharding Plan:
  - Phase 1: single cluster with read replicas per region; 
  - Phase 2: logical shards by region or hash on user_id for massive scale; 
  - Consistency: prefer strong consistency per shard; use saga/outbox patterns for cross-shard workflows.
- Caching:
  - Redis for hot paths: user sessions (if used), allocation candidates, NGO capacity snapshots, map results.
- Search:
  - OpenSearch for searchable facets and geo queries; backfilled via Kafka connect.
- CDC/Streaming:
  - Debezium → Kafka → 
    - OpenSearch indexers
    - Snowflake Snowpipe/Kafka connector
    - Projections/caches
- Analytics:
  - Snowflake as warehouse for BI (or Databricks/EMR if preferring Spark/Scala); dashboards via QuickSight/Looker.

Realtime Architecture at Scale
- Connections: 10M+ concurrent channels require strong horizontal scaling.
- Option A (managed): AWS API Gateway WebSockets + Lambda integration + DynamoDB for connection state; simplifies scale but limits custom protocol features.
- Option B (self-managed recommended):
  - Real-Time Gateway (Go) on EKS; sticky sessions by connection id; shard by consistent hashing.
  - Backplane: NATS JetStream for low-latency fanout (or Kafka for persistence + Redis for local hot fanout).
  - Use protobuf payloads; message budget and quotas per tenant; backpressure and slow consumer handling.
- Protocol:
  - Use native WebSocket with protobuf frames; at edge provide Socket.IO compatibility if needed.
- Security:
  - Connection JWTs short-lived; re-auth on rotate; permissioned topics; e2e signature if necessary.

Allocation/Optimization Strategy
- Replace “spawn Python + temp files” with:
  - gRPC Optimization Engine (C++ OR-Tools). Container with CPU and memory limits; optionally GPU nodes if needed in future.
  - Request flow: Orchestrator API validates and publishes job to Kafka/SQS → Workers consume, compute, and publish results to a result topic/table. Or orchestrate via Step Functions for retries and timeouts.
  - Config flags: min allocation, max sources, capacity, expiry, distance, preference weights, fairness/recency curves.
  - Versioning: Strategy version and solver version recorded for auditability.
  - Observability: Per-job metrics, solver timing, constraint violations, input size histograms.

3D Models and Frontend Performance Plan
- Asset Pipeline:
  - Convert FBX → GLB (glTF 2.0) via Blender CLI.
  - Apply gltfpack/meshoptimizer for geometry compression.
  - Texture compression with KTX2 Basis Universal; generate mipmaps; limit texture sizes (e.g., 2k max).
  - Generate LODs and impostors for distant objects.
  - Store variants (mobile/desktop, low/med/high poly).
  - Automation: Lambda-backed S3 event triggers or Step Functions to run optimization jobs in containerized workers.
- Delivery:
  - Host on S3 + CloudFront; enable range requests; set aggressive caching; immutable versioned URLs.
  - Lazy-load models; prefetch on hover/idle; partial loading (draco).
  - Use React Suspense and code-splitting; Web Workers for heavy parsing.
- Rendering:
  - Use @react-three/fiber with Drei helpers; cull off-screen meshes; limit lights/shadows; use instancing where possible.
  - FPS budgets per device; dynamic quality scaler.
- State/Data:
  - Standardize on @tanstack/react-query v5 + Redux Toolkit for global app state. Remove react-query v3 and duplicate libs.
  - Real-time via WS client with reconnect, backoff, and topic auth.
- Maps:
  - Standardize to one mapping stack (Google or Leaflet + OpenStreetMap). Cache geocoding results server-side.

API and Contract Design
- Protobuf for internal gRPC:
  - user.proto, donor.proto, ngo.proto, donation.proto, allocation.proto, driver.proto, realtime.proto, notification.proto, search.proto.
  - Use Buf (build, breaking change checks, lint); polyglot generation (Go/TS/Python/C++).
- REST/GraphQL at the edge (OpenAPI/GraphQL schema-first).
- Validation with Zod/Protobuf schemas; error taxonomy; idempotency keys for writes; pagination/filters standardized.

Infrastructure as Code (AWS CDK)
- Accounts: dev, staging, prod; separate shared services (Kafka, observability).
- Stacks:
  - Network: VPC, subnets, NAT, endpoints.
  - Compute: EKS (managed node groups or Fargate).
  - Data: Aurora Postgres, ElastiCache Redis, OpenSearch, MSK (Kafka), S3.
  - Edge: CloudFront, Route53, ACM, WAF, API Gateway.
  - Security: IAM, Secrets Manager, KMS, guardrails.
  - Observability: Managed Prometheus/Grafana or self-hosted; OTEL collectors.
- CDK Pipelines to deploy environments with approvals and automated testing.

CI/CD and DevEx
- Repos:
  - Monorepo (pnpm + Turborepo) for FE/BFF/shared libs; multi-repo for heavy services (Optimization C++, Real-time Go).
- Pipelines (GitHub Actions):
  - Build, lint, test (unit/integration/contract), build Docker, SBOM + vulnerability scan (Trivy/Grype), push to ECR.
  - Deploy via ArgoCD/Flux (GitOps) to EKS; progressive delivery (Argo Rollouts/Flagger).
  - Canary, blue/green; automatic rollback on SLO regressions.
- Testing:
  - Contract tests with Pact; e2e with Playwright/Cypress; k6 for load testing; chaos with LitmusChaos.
- Local Dev:
  - Tilt/Skaffold; Docker Compose for minimal stack; testcontainers for integration tests.

Security and Compliance
- AuthN/Z:
  - OIDC with Cognito/Auth0/Keycloak; service-to-service mTLS via mesh identities (SPIFFE/SPIRE).
- Data:
  - PII encryption at rest (KMS) and in transit; field-level encryption for sensitive columns.
  - Secrets in AWS Secrets Manager/Vault; automatic rotation.
- Edge Security:
  - WAF rules (rate limits, bot control, OWASP rules), DDoS (Shield Advanced).
- AppSec:
  - SAST (CodeQL), DAST (ZAP), dependency scanning; container image signing (cosign).
- Audit & Governance:
  - CloudTrail, audit logs, access reviews, SoD; GDPR/CCPA workflows.

Migration Strategy (Strangler Pattern)
- Phase 0 – Foundation (Weeks 0–4):
  - Containerize current backend; move DB to managed Postgres (Aurora PG). Introduce Redis cluster (ElastiCache). Set up CDK baseline infra for dev/stage.
  - Implement proper db pool; extract configuration; centralize logging and metrics.
  - Stand up OpenTelemetry, Prometheus, Grafana, Loki, Sentry.
- Phase 1 – Edge and Auth (Weeks 4–8):
  - Introduce API Gateway and BFF; integrate OIDC; migrate clients to Gateway URLs. Set up WAF and rate limiting at edge.
- Phase 2 – Real-time Gateway (Weeks 6–12):
  - Build Go WebSocket service with NATS backplane; migrate any existing WS usage (trackingServer) to new gateway; deprecate Socket.IO if not required.
- Phase 3 – Allocation Service (Weeks 8–16):
  - Build C++ OR-Tools gRPC service + Orchestrator service; replace spawn-python path. Define protobuf contracts; add job queue; add per-job metrics.
- Phase 4 – Domain Services (Weeks 10–20):
  - Extract Donor, NGO, Donation, Orders/Fulfillment, Driver services. Implement gRPC between services; Postgres schemas per service; start event sourcing/outbox where needed.
- Phase 5 – Search and Analytics (Weeks 14–22):
  - Stand up OpenSearch indices; set up Debezium CDC to Kafka → OpenSearch and Snowflake. Build analytics dashboards.
- Phase 6 – Micro-frontend (Weeks 12–24):
  - Implement Module Federation (Webpack 5) or Vite MF plugin. Split apps: Landing, Donor Portal, NGO Portal, Driver App, Admin, 3D Gallery. Create shared design system. Standardize React Query v5 + Redux Toolkit.
- Phase 7 – 3D Optimization Pipeline (Weeks 12–24):
  - Implement S3-triggered asset processing: Blender CLI → gltfpack/meshoptimizer → ktx2; LOD generation; publish to versioned S3 paths and CDN.
- Phase 8 – Multi-region and DR (Weeks 20–28):
  - Add read replicas/Global Database for Aurora; multi-region Kafka; cross-region S3 replication; active-active or active-passive failover with Route53 health checks and traffic policies.
- Phase 9 – Hardening and Scale (Weeks 24–32):
  - Load tests to 500k+ RPS; chaos engineering; resiliency drills; cost optimization; finalize SLOs and error budgets.

Data Model and Eventing (Illustrative)
- Key Entities: User, Donor, NGO, Donation, Allocation, Order, Driver, Route, Notification, ChatMessage.
- Topics (Kafka):
  - user.created, donor.verified, donation.created/updated, allocation.requested/completed, order.created/status_changed, driver.location.updated, notification.sent, search.index.update.
- Outbox pattern from OLTP tables into Kafka; use Protobuf schemas and schema registry.

C++ and Go Usage
- C++:
  - Optimization Engine with OR-Tools C++ API for maximum performance. gRPC with protobuf; containerized and auto-scaled. Optional SIMD/BLAS optimizations in image/geo computations if needed.
- Go:
  - Real-time Gateway (WS), Driver/Logistics, Routing/Maps aggregator. gRPC-first design; high concurrency; memory-safe compared to C++ in services.
- Node/TS:
  - BFFs, edge adapters, business logic that benefits from ecosystem speed. Shared libraries.

WebSockets Implementation Detail
- Topics:
  - order:{id}, driver:{id}, ngo:{id}, user:{id}, announcements:{region}.
- Auth:
  - Short-lived connection tokens; authorize join to topics. Revalidate on subscription.
- Scale:
  - Shard connections; horizontal pods; use NATS JetStream streams per topic category.
- Reliability:
  - At-most-once to clients; at-least-once from producers into backplane. Client acks optional for critical flows.

3D Asset Pipeline Detail
- Conversion:
  - FBX → GLB via Blender (headless). Scripts in container.
- Optimization:
  - gltfpack: -cc -kn -tc -vt -mi -v.
  - meshoptimizer for vertex/index reordering; KTX2 encoding for textures (BasisU).
- LOD:
  - Auto LOD creation using Blender Decimate/Remesh; link via EXT_mesh_gpu_instancing where applicable.
- Delivery:
  - Version objects by content hash; immutable URLs; CloudFront cache policies; client hints for resolution.

Observability & SRE
- Metrics:
  - RED/USE metrics per service; business KPIs (allocations/hour, on-time pickups).
- Traces:
  - End-to-end across Gateway→BFF→Service→DB/Cache→gRPC calls; baggage for tenant/correlation ids.
- Logs:
  - JSON structured; correlation ids; retention policies; PII redaction.
- Runbooks:
  - Per service SLO, dashboards, alerts, on-call rotations; incident management playbooks.

Security Controls
- Input validation via Zod/proto; central error taxonomy to avoid info leaks.
- mTLS for gRPC; SPIFFE ids; policy enforcement (OPA/Gloo).
- Data retention and delete workflows for compliance.
- Pen testing cadence and automated scanning.

Cost and Capacity Plan (initial)
- Region: us-east-1 primary, eu-west-1 secondary (example).
- Start with:
  - Aurora PG Srvls v2 (2–8 ACUs), ElastiCache 3-shard, MSK 3-broker (dev/stage), EKS managed nodes (c5,g4dn for 3D workers if needed), OpenSearch small cluster (dev).
- Iterate via load tests and scale up with autoscaling.

Risk Register (selected)
- OR-Tools rewrite risk in C++: mitigate with parallel Python service fallback initially.
- Realtime fanout spikes: use quota/backpressure, managed fallback (Ably/Pusher) for peak events.
- 3D asset sizes on low-end devices: adaptive delivery, device profiling, LOD gating.
- Data consistency across services: adopt outbox/CDC rigorously; contract testing.

Initial Backlog (Epics → High-level Stories)
- EPIC: Foundation & Observability
  - Containerize monolith; add proper PG pool; OTEL SDKs; central logs; error tracking.
- EPIC: Auth & Edge
  - OIDC integration; API Gateway/WAF; token propagation; RBAC policies.
- EPIC: Realtime Gateway
  - Go WS service; NATS cluster; client SDK; migrate trackingServer.
- EPIC: Allocation Service
  - Protobuf schema; C++ OR-Tools service; job queue; API orchestration; result persistence; admin tooling.
- EPIC: Domain Services
  - Extract Donor, NGO, Donation, Orders, Driver; per-service DB schemas; eventing.
- EPIC: Search
  - OpenSearch setup; indexers; query APIs; FE integration.
- EPIC: Micro-frontend
  - MF architecture; shared design system; route-level federation; FE state standardization.
- EPIC: 3D Pipeline
  - S3 triggers; conversion and compression; CDN rollout; runtime quality scaler.
- EPIC: Analytics
  - Kafka → Snowflake; events schema; BI dashboards; feature store POC.

Actionable 90-Day Plan (milestones)
- 0–30 days:
  - CDK bootstrap; Dev/Stg envs. Containerize current backend. Fix PG Pool (singleton, pooled clients). Centralized logging/metrics/tracing. Basic CI/CD with image scanning. S3 for media; signed URLs.
- 31–60 days:
  - API Gateway + WAF; OIDC; BFFs. Realtime Gateway MVP in Go with NATS; migrate basic tracking. Allocation Orchestrator API; Python service behind a queue as interim. Start OpenSearch indexers. FE standardize React Query v5; remove react-query v3.
- 61–90 days:
  - C++ Optimization Engine gRPC service; cutover from Python. Extract Donor/NGO/Donation services. 3D asset pipeline automated + CDN rollout. Load tests to validate 10k→50k sustained RPS per service. Begin multi-region DR setup.

Mappings and Cleanups (from codebase)
- DB Util: Replace per-query Pool creation with shared singleton and connection pooling. Add retries, timeouts, and circuit breaker.
- Redis Client: Configure via env; TLS/auth; connection reuse; health checks.
- WebSockets: Replace placeholder trackingServer with Real-time Gateway integration. Use a client library and topic structure.
- food-allocation-milp route: Remove file-based Python spawning. Call Orchestrator API (gRPC/REST) which enqueues to Optimization Service.
- Frontend deps: Remove duplicate react-query; standardize @tanstack/react-query v5. Consolidate mapping library usage.
- ML artifacts in repo (rf.pkl, etc.): Move to model registry/storage; load via versioned URLs; don’t commit binaries.

Acceptance Criteria for This Plan
- Plan reviewed and approved by Eng Leads, SRE, Security, and Product.
- Budgets and SLOs acknowledged by leadership.
- 90-day milestones entered into tracking (Jira) with owners.

Appendix A – Example Protobuf (allocation.proto excerpt)
syntax = "proto3";
package allocation.v1;

message NGO {
  int64 id = 1;
  double storage_capacity = 2;
  double latitude = 3;
  double longitude = 4;
  int32 priority_level = 5;
  repeated string food_preferences = 6;
}

message FoodDonation {
  int64 id = 1;
  double remaining_quantity = 2;
  string food_type = 3;
  double latitude = 4;
  double longitude = 5;
  int64 expiration_epoch_ms = 6;
}

message AllocationRequest {
  repeated NGO ngos = 1;
  repeated FoodDonation foods = 2;
  string strategy_version = 3;
}

message Allocation {
  int64 ngo_id = 1;
  int64 food_donation_id = 2;
  double allocated_quantity = 3;
}

message AllocationResponse {
  string status = 1; // optimal,infeasible,timeout
  double objective_value = 2;
  repeated Allocation allocations = 3;
}

service OptimizationEngine {
  rpc Solve (AllocationRequest) returns (AllocationResponse);
}

Appendix B – Kafka Topics (illustrative)
- donation.created v1
- allocation.requested v1
- allocation.completed v1
- order.created v1
- order.status.changed v1
- driver.location.updated v1
- search.index.update v1
- user.created v1

Appendix C – 3D Tooling
- Blender CLI for FBX→GLB: blender -b -P convert.py -- input.fbx output.glb
- gltfpack: gltfpack -i in.glb -o out.glb -cc -kn -tc -vt -mi
- meshoptimizer & ktx2/basisu
- Validation via glTF Validator; automated in CI.

Appendix D – Observability SLO Dashboards
- Availability, latency percentiles, error budget burn, queue depths, Kafka lag, DB connections, cache hit ratio, GC/memory, WS connection counts, fanout times, FE Core Web Vitals.

End of document.
